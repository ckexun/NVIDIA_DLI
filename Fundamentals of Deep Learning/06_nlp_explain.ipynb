{"cells":[{"cell_type":"markdown","metadata":{"id":"AfMQ7rFMr-yB"},"source":["<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8SBOl9BWr-yH"},"source":["# 6. 自然語言處理(Natural Language Processing)"]},{"cell_type":"markdown","metadata":{"id":"li_QSLlFr-yI"},"source":["在本教程中，我們將從各自獨立的數據片段（如靜態圖像）轉向依賴於序列中其他數據項的資料。在我們的例子中，我們將使用文字語句(text sentences)。語言自然由序列數據組成，以單詞中的字元(characters)和句子中的單詞(word)的形式呈現。序列數據的其他例子包括隨時間變化的股票價格和天氣數據。影片雖然包含靜態圖像，但也是序列。數據中的元素與前後的內容有關聯，這種狀況需要採取不同的方法。"]},{"cell_type":"markdown","metadata":{"id":"mnncngjRr-yJ"},"source":["## 6.1 目標(Objectives)"]},{"cell_type":"markdown","metadata":{"id":"QakLkdgrr-yJ"},"source":["-   使用符記器(tokenizer)準備文本(text)以供神經網路使用\n","-   了解如何使用內嵌(Embedding)來識別文本數據的數值特徵"]},{"cell_type":"markdown","metadata":{"id":"vdcQjBPTr-yK"},"source":["## 6.2 BERT\n"]},{"cell_type":"markdown","metadata":{"id":"GNwC96IOr-yK"},"source":["\n","BERT，全名為雙向編碼器表示法轉換器(Bidirectional Encoder Representations from Transformers)，是2018年由[Google](https://www.google.com/)推出的一個突破性模型。\n","\n","BERT同時針對兩個目標進行訓練：\n","\n","-   從一系列單詞(sequence of words)中預測缺失的單詞\n","-   在一系列句子(sequence of sentences)之後預測新句子\n","\n","讓我們看看BERT如何應對這兩種類型的挑戰。"]},{"cell_type":"markdown","metadata":{"id":"3bGu151Xr-yL"},"source":["## 6.3 符記化(Tokenization)"]},{"cell_type":"markdown","metadata":{"id":"zotVHiTYr-yL"},"source":["由於神經網路是數字運算機器，讓我們將文本(text)轉換為數值符記(numerical tokens)。讓我們載入BERT的[符記器(tokenizer)](https://huggingface.co/docs/transformers/main_classes/tokenizer#tokenizer)："]},{"cell_type":"markdown","metadata":{"id":"HEa5dgy6r-yM"},"source":["### ***bert-base-cased***\n","一個純英文版本的 BERT詞彙表，而且區分大小寫"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYNFd0DJr-yM","outputId":"766e845a-54db-4301-a4e1-4f2d6a678e41"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["import torch\n","from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForQuestionAnswering\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"]},{"cell_type":"markdown","metadata":{"id":"-SuTngCxr-yP"},"source":["BERT的`tokenizer`可以一次[編碼(encode)](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode)多個文本。我們稍後將測試BERT的記憶力，所以讓我們給它一些資訊和一個關於該資訊的問題。歡迎稍後回到這裡嘗試不同的句子組合。"]},{"cell_type":"markdown","metadata":{"id":"7iQoafM1r-yP"},"source":["### ***tokenizer.encode 說明***\n","會先把文字 切成「符記」(token)，也就是英文的單字、詞根或片段（有些會像是 \"quadra\", \"##tical\"）\n","\n","然後 轉成一串對應的編號（數字）\n","\n","並且加上 BERT 所需要的「特殊符號」：\n","\n","    [CLS]：句子的開頭\n","\n","    [SEP]：每一句話的結尾\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaGD-Cv8r-yP","outputId":"da709e17-92ad-4c90-e42d-5cb1f33283e9"},"outputs":[{"data":{"text/plain":["[101,\n"," 146,\n"," 2437,\n"," 11838,\n"," 117,\n"," 1241,\n"," 1103,\n"," 3014,\n"," 1105,\n"," 186,\n"," 18413,\n"," 21961,\n"," 1348,\n"," 119,\n"," 102,\n"," 1327,\n"," 1912,\n"," 1104,\n"," 11838,\n"," 1202,\n"," 146,\n"," 2437,\n"," 136,\n"," 102]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["text_1 = \"I understand equations, both the simple and quadratical.\"\n","text_2 = \"What kind of equations do I understand?\"\n","\n","# Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n","#把 text_1 和 text_2 這兩句英文，經過 tokenizer 處理，變成一串數字（token IDs），並自動加上 BERT 需要的特殊符號。\n","indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)\n","indexed_tokens"]},{"cell_type":"markdown","metadata":{"id":"cFHC3Ry5r-yQ"},"source":["上組數字的意思是：\n","\n","    -101 是 [CLS]\n","    -後面是第一句 text_1 的編碼\n","    -102 是 [SEP]\n","    -接著是第二句 text_2 的編碼\n","    -最後再一個 102 結尾（結束整個輸入）"]},{"cell_type":"markdown","metadata":{"id":"FXSUHO5Dr-yQ"},"source":["如果我們計算符記(token)的數量，會發現符記(token)比我們句子中的單詞更多。讓我們看看為什麼會這樣。我們可以使用[convert_ids_to_tokens](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.convert_ids_to_tokens)來查看使用了哪些符記(token)。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pDd6T1AQr-yQ","outputId":"877f1633-8c9b-4c7d-e224-da5d51d672e4"},"outputs":[{"data":{"text/plain":["['[CLS]',\n"," 'I',\n"," 'understand',\n"," 'equations',\n"," ',',\n"," 'both',\n"," 'the',\n"," 'simple',\n"," 'and',\n"," 'q',\n"," '##uad',\n"," '##ratic',\n"," '##al',\n"," '.',\n"," '[SEP]',\n"," 'What',\n"," 'kind',\n"," 'of',\n"," 'equations',\n"," 'do',\n"," 'I',\n"," 'understand',\n"," '?',\n"," '[SEP]']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_ids_to_tokens([str(token) for token in indexed_tokens])"]},{"cell_type":"markdown","metadata":{"id":"lr_D67krr-yQ"},"source":["\n","索引列表(indexed list)比我們原始輸入更長的原因有兩個：\n","\n","1.  `tokenizer`添加了`special_tokens`來表示序列(sequence)的開始(`[CLS]`)和句子之間的分隔(`[SEP]`)。\n","2.  `tokenizer`可以將一個單詞分解成多個部分。\n","\n","從語言學(linguistic)的角度來看，第二點很有趣。許多語言都有[詞根(word roots)](https://en.wikipedia.org/wiki/List_of_Greek_and_Latin_roots_in_English)，或構成單詞的組件。例如，\"quadratic\"這個詞有詞根\"quadr\"，意思是\"4\"。BERT不是使用語言定義的詞根，而是使用[WordPiece](https://paperswithcode.com/method/wordpiece)模型來尋找如何分解單詞的模式。我們今天將使用的BERT模型有`28996`個符記(token)詞彙(vocabulary)。"]},{"cell_type":"markdown","metadata":{"id":"_5Aio-QWr-yR"},"source":["\n","如果我們想直接[解碼(decode)](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)我們編碼的文本，我們可以做到。注意`special_tokens`已被添加進去。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDVnSytEr-yR","outputId":"f5fbd93d-ce6a-465b-b801-604198ffb572"},"outputs":[{"data":{"text/plain":["'[CLS] I understand equations, both the simple and quadratical. [SEP] What kind of equations do I understand? [SEP]'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(indexed_tokens)"]},{"cell_type":"markdown","metadata":{"id":"lF4Biub-r-yR"},"source":["\n","## 6.4 文本分段(Segmenting Text)"]},{"cell_type":"markdown","metadata":{"id":"ekKPYpCSr-yR"},"source":["\n","為了使用BERT模型進行預測，它還需要一個`segment_ids`列表。這是一個與我們的符記(token)長度相同的向量，表示每個segment屬於哪個段落。\n","\n","由於我們的`tokenizer`添加了一些`special_tokens`，我們可以使用這些特殊符記(token)來找到segment。首先，讓我們定義哪個索引對應哪個特殊符記(token)。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNkO_kMlr-yS"},"outputs":[],"source":["cls_token = 101\n","sep_token = 102"]},{"cell_type":"markdown","metadata":{"id":"zKiTXIEzr-yS"},"source":["接下來，我們可以創建一個`for`迴圈。我們將從`segment_id`設置為`0`開始，每當我們看到[SEP]符記時，我們就增加`segment_id`。為了確保萬無一失，我們將`segment_ids`和`indexd_tokens`都作為張量(tensors)傳回，因為我們稍後會將這些輸入到模型中。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLHDPdR1r-yS"},"outputs":[],"source":["def get_segment_ids(indexed_tokens):\n","    segment_ids = []\n","    segment_id = 0\n","    for token in indexed_tokens:\n","        #segment_ids 是我們要建立的「段落 ID 列表」\n","        #segment_id 是一個計數器，初始值為 0，表示「目前是在第幾句話」\n","        if token == sep_token:\n","            segment_id += 1\n","        segment_ids.append(segment_id)\n","    segment_ids[-1] -= 1  # Last [SEP] is ignored(忽略最後一個[SEP])\n","    return torch.tensor([segment_ids]), torch.tensor([indexed_tokens])"]},{"cell_type":"markdown","metadata":{"id":"yu4qpjpXr-yS"},"source":["\n","讓我們測試一下。每個數字是否正確對應第一句和第二句？\n","\n","segment_id = 0 表示 第一句（text_1）\n","\n","segment_id = 1 表示 第二句（text_2）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOT5qmVmr-yT","outputId":"57eb46f5-6507-4bec-9714-2885d93a9e21"},"outputs":[{"data":{"text/plain":["tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["segments_tensors, tokens_tensor = get_segment_ids(indexed_tokens)\n","segments_tensors"]},{"cell_type":"markdown","metadata":{"id":"1uep7xc8r-yT"},"source":["## 6.4 文本遮罩(Text Masking)"]},{"cell_type":"markdown","metadata":{"id":"BCsBzE2Lr-yT"},"source":["\n","讓我們從BERT對單詞的關注開始。為了訓練詞內嵌(word embeddings)，BERT在一系列單詞中遮蔽(mask out)一個單詞。遮罩(mask)有它自己的特殊符記(token)："]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz0r2gxcr-yT","outputId":"3630e6d4-eff2-41a0-816c-d94d9402baaf"},"outputs":[{"data":{"text/plain":["'[MASK]'"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.mask_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTbjcZbJr-yU","outputId":"4e1711cd-c340-44a3-9769-d2291cb46cee"},"outputs":[{"data":{"text/plain":["103"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.mask_token_id"]},{"cell_type":"markdown","metadata":{"id":"fHzdLmVTr-yU"},"source":["\n","讓我們取之前的兩個句子，遮蔽索引`5`的位置。歡迎回到這裡更改索引以查看結果如何變化！"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CulhvV8jr-yU"},"outputs":[],"source":["masked_index = 5"]},{"cell_type":"markdown","metadata":{"id":"KqUvVeLBr-yU"},"source":["\n","接下來，我們將應用遮罩並驗證它是否出現在我們的句子序列中。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoZy88N5r-yV","outputId":"8402bd74-85ef-48ed-c33a-b14d818d93cf"},"outputs":[{"data":{"text/plain":["'[CLS] I understand equations, [MASK] the simple and quadratical. [SEP] What kind of equations do I understand? [SEP]'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["indexed_tokens[masked_index] = tokenizer.mask_token_id\n","tokens_tensor = torch.tensor([indexed_tokens])\n","tokenizer.decode(indexed_tokens)"]},{"cell_type":"markdown","metadata":{"id":"C-5lZLy0r-yV"},"source":["\n","然後，我們將載入用於預測被遮蔽單詞的模型：`modelForMaskedLM`。\n","\n","從 Hugging Face 載入一個已經訓練好的 BERT 模型，用來做「填空」任務（Masked Language Modeling）"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehSeZFDNr-yV","outputId":"4f443fa7-26a7-4ded-b2f4-bb55ac314156"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["masked_lm_model = BertForMaskedLM.from_pretrained(\"bert-base-cased\")"]},{"cell_type":"markdown","metadata":{"id":"GJPUaQ-fr-yW"},"source":["\n","\n","就像其他PyTorch模組一樣，我們可以檢查架構。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B59p_yjor-yW","outputId":"0c4e7610-acc0-4944-c362-e270a832aef2"},"outputs":[{"data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n","    )\n","  )\n",")"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["masked_lm_model"]},{"cell_type":"markdown","metadata":{"id":"UaPKc-8Qr-yk"},"source":["\n","你能發現標記為`word_embeddings`的部分嗎？這些是BERT從一個個符記(token)學習到的內嵌(Embedding)。\n","\n","取得 BERT 模型中「詞嵌入層（word embedding layer）」的參數表，也就是把每個詞轉成向量的「對照表」。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0Qf08ARr-yk","outputId":"8a42ec50-6268-4ace-d916-69dbea2057be"},"outputs":[{"data":{"text/plain":["Parameter containing:\n","tensor([[-0.0005, -0.0416,  0.0131,  ..., -0.0039, -0.0335,  0.0150],\n","        [ 0.0169, -0.0311,  0.0042,  ..., -0.0147, -0.0356, -0.0036],\n","        [-0.0006, -0.0267,  0.0080,  ..., -0.0100, -0.0331, -0.0165],\n","        ...,\n","        [-0.0064,  0.0166, -0.0204,  ..., -0.0418, -0.0492,  0.0042],\n","        [-0.0048, -0.0027, -0.0290,  ..., -0.0512,  0.0045, -0.0118],\n","        [ 0.0313, -0.0297, -0.0230,  ..., -0.0145, -0.0525,  0.0284]],\n","       requires_grad=True)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["embedding_table = next(masked_lm_model.bert.embeddings.word_embeddings.parameters())\n","embedding_table"]},{"cell_type":"markdown","metadata":{"id":"pZKEq8Jor-yk"},"source":["\n","我們可以驗證BERT詞彙表中的`28996`個符記(token)，每一個都有大小為`768`的內嵌(Embedding)或 768 維的向量。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ku7YvxjJr-yk","outputId":"c91c2bac-6263-43d6-aa4e-2eb797b01e68"},"outputs":[{"data":{"text/plain":["torch.Size([28996, 768])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["embedding_table.shape"]},{"cell_type":"markdown","metadata":{"id":"5imeQLn4r-yk"},"source":["\n","讓我們測試模型！它能正確預測我們提供的句子中缺失的單詞(word)嗎？我們將使用[torch.no_grad](https://pytorch.org/docs/stable/generated/torch.no_grad.html)來告知PyTorch不要計算梯度。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uh5rOBoVr-yl","outputId":"11b26362-62d9-4cd9-ce93-f9076019ec88"},"outputs":[{"data":{"text/plain":["MaskedLMOutput(loss=None, logits=tensor([[[ -7.3832,  -7.2504,  -7.4539,  ...,  -6.0597,  -5.7928,  -6.2133],\n","         [ -6.7681,  -6.7896,  -6.8317,  ...,  -5.4655,  -5.4048,  -6.0683],\n","         [ -7.7323,  -7.9597,  -7.7348,  ...,  -5.7611,  -5.3566,  -4.3361],\n","         ...,\n","         [ -6.1213,  -6.3311,  -6.4144,  ...,  -5.8884,  -4.1157,  -3.1189],\n","         [-12.3216, -12.4479, -11.9787,  ..., -10.6539,  -8.7396, -11.0487],\n","         [-13.4115, -13.7876, -13.5183,  ..., -10.6359, -11.6582, -10.9009]]]), hidden_states=None, attentions=None)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["with torch.no_grad():\n","    predictions = masked_lm_model(tokens_tensor, token_type_ids=segments_tensors)\n","predictions"]},{"cell_type":"markdown","metadata":{"id":"qVlVESenr-yl"},"source":["這有點難以閱讀，讓我們看看`shape`以更好地了解發生了什麼。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYrLTypHr-yl","outputId":"ea9e9472-46aa-492a-e8ad-3158e7f52bd8"},"outputs":[{"data":{"text/plain":["torch.Size([1, 24, 28996])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["predictions[0].shape"]},{"cell_type":"markdown","metadata":{"id":"9WopGLhPr-yl"},"source":["`24`是我們的符記(token)數量，`28996`是BERT詞彙表中每個符記(token)的預測。我們想在所有詞彙表符記(token)中找到最高值，所以我們可以使用[torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html)來找到它。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ak0fuT0mr-yl","outputId":"cc4f159f-9f93-47ba-83cc-b6a6324c7626"},"outputs":[{"data":{"text/plain":["1241"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Get the predicted token\n","predicted_index = torch.argmax(predictions[0][0], dim=1)[masked_index].item()\n","predicted_index"]},{"cell_type":"markdown","metadata":{"id":"96RqNccwr-yl"},"source":["\n","讓我們看看符記(token)`1241`對應的是什麼："]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vERSihL_r-ym","outputId":"40e5b619-db3a-4269-b140-1c6854a6bba8"},"outputs":[{"data":{"text/plain":["'both'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n","predicted_token"]},{"cell_type":"markdown","metadata":{"id":"8JxKnKNgr-ym"},"source":["\n","你覺得呢？是正確的嗎？"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1mX4c1nr-ym","outputId":"73aa93fe-6d73-4736-b121-7efcf457df48"},"outputs":[{"data":{"text/plain":["'[CLS] I understand equations, [MASK] the simple and quadratical. [SEP] What kind of equations do I understand? [SEP]'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.decode(indexed_tokens)"]},{"cell_type":"markdown","metadata":{"id":"PpJGslVyr-ym"},"source":["## 6.5 問與答(Question and Answering)執行閱讀任務"]},{"cell_type":"markdown","metadata":{"id":"eD6GX7eQr-ym"},"source":["\n","雖然單詞遮罩很有趣，但BERT設計用於更複雜的問題，如句子預測(sentence prediction)。它能夠通過建立在[注意力轉換器(Attention Transformer)](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)架構上來實現這一點。\n","\n","我們將在本節中使用不同版本的BERT，它有自己的分詞器(tokenizer)。讓我們為我們的範例句子找到一組新的符記(token)。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttXHGHeCr-ym"},"outputs":[],"source":["text_1 = \"I understand equations, both the simple and quadratical.\"\n","text_2 = \"What kind of equations do I understand?\"\n","\n","#使用問答專用的 tokenizer，載入一個為問答任務（SQuAD 資料集）特別訓練過的 BERT tokenizer\n","question_answering_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","\n","#編碼兩段文字為 token ID\n","indexed_tokens = question_answering_tokenizer.encode(text_1, text_2, add_special_tokens=True)\n","\n","#定義 segment IDs，區分文章(text_1)與問題(text_2)\n","segments_tensors, tokens_tensor = get_segment_ids(indexed_tokens)"]},{"cell_type":"markdown","metadata":{"id":"QlkZX4A3r-ym"},"source":["\n","接下來，讓我們載入`question_answering_model`。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyYqjVoIr-yn","outputId":"8da3c517-2ed0-46ff-e055-aee342a0113b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["#載入一個已經訓練好、專門用來做問答（Question Answering）任務的 BERT 模型\n","question_answering_model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"]},{"cell_type":"markdown","metadata":{"id":"1Wc5qoZtr-yn"},"source":["\n","我們可以輸入我們的符記(token)和段落(segments)，就像我們遮蔽一個單詞時一樣。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eC_ewMV8r-yn","outputId":"d186f596-e811-45b4-96d8-a4a21137e035"},"outputs":[{"data":{"text/plain":["QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-5.5943, -4.2960, -5.2682, -1.2511, -6.8350, -0.3992,  2.2274,  2.4654,\n","         -6.6066,  2.5014, -4.4613, -4.8040, -7.8383, -5.5944, -4.7833, -6.9730,\n","         -7.1477, -5.2967, -7.4825, -6.7737, -6.8806, -8.6612, -5.5944]]), end_logits=tensor([[-0.7409, -5.3478, -4.2317, -0.0275, -2.6293, -5.9589, -2.8828,  2.7770,\n","         -4.8512, -2.2092, -2.2413,  4.4412, -0.7181, -0.7411, -3.8988, -5.3865,\n","         -5.0452, -4.4974, -6.3098, -5.5938, -5.5562, -5.3034, -0.7412]]), hidden_states=None, attentions=None)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Predict the start and end positions logits\n","#不需要計算梯度（不用訓練），只是做「推論」\n","with torch.no_grad():\n","    out = question_answering_model(tokens_tensor, token_type_ids=segments_tensors)\n","out"]},{"cell_type":"markdown","metadata":{"id":"087S6ZkCr-yn"},"source":["\n","`question_answering_model`和回答(answering)模型正在掃描我們輸入的序列資料，以找到最能回答問題的子序列。`start_logits`算出的值越高，答案的段落由此開始的可能性就越大。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpqpK4hhr-yn","outputId":"2123333f-f25f-4016-afa1-da2a3b7b06a5"},"outputs":[{"data":{"text/plain":["tensor([[-5.5943, -4.2960, -5.2682, -1.2511, -6.8350, -0.3992,  2.2274,  2.4654,\n","         -6.6066,  2.5014, -4.4613, -4.8040, -7.8383, -5.5944, -4.7833, -6.9730,\n","         -7.1477, -5.2967, -7.4825, -6.7737, -6.8806, -8.6612, -5.5944]])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["out.start_logits"]},{"cell_type":"markdown","metadata":{"id":"mWkPhKmlr-yn"},"source":["\n","同樣，`end_logits`中的值越高，答案在該符記(token)位置結束的可能性就越大。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJsCglXzr-yn","outputId":"88d46dc1-6123-4ede-c747-4f2b3bef7a8f"},"outputs":[{"data":{"text/plain":["tensor([[-0.7409, -5.3478, -4.2317, -0.0275, -2.6293, -5.9589, -2.8828,  2.7770,\n","         -4.8512, -2.2092, -2.2413,  4.4412, -0.7181, -0.7411, -3.8988, -5.3865,\n","         -5.0452, -4.4974, -6.3098, -5.5938, -5.5562, -5.3034, -0.7412]])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["out.end_logits"]},{"cell_type":"markdown","metadata":{"id":"4RPj5kQHr-yo"},"source":["\n","然後我們可以使用[torch.argmax](https://pytorch.org/docs/stable/generated/torch.argmax.html)來透過起始與結束位置得到`answer_sequence`："]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0nfsz9vr-yo","outputId":"cea2e898-42f9-457e-f6d8-9ae826e84780"},"outputs":[{"data":{"text/plain":["[17718, 23671, 2389]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["answer_sequence = indexed_tokens[torch.argmax(out.start_logits):torch.argmax(out.end_logits)+1]\n","answer_sequence"]},{"cell_type":"markdown","metadata":{"id":"hbYekgEyr-yo"},"source":["\n","最後，讓我們[解碼(decode)](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)這些符記(token)，看看答案是否正確！"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELP5AaNor-yo","outputId":"c03b85c7-3055-44cd-8866-f08a5689b11a"},"outputs":[{"data":{"text/plain":["['quad', '##ratic', '##al']"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["#把 token IDs 轉成 token\n","question_answering_tokenizer.convert_ids_to_tokens(answer_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQnyTqVVr-yo","outputId":"fa0dc47e-00c0-482e-9ad4-b29c88c64480"},"outputs":[{"data":{"text/plain":["'quadratical'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["#把 token 組合成一句話\n","question_answering_tokenizer.decode(answer_sequence)"]},{"cell_type":"markdown","metadata":{"id":"5DoUSHbLr-yo"},"source":["#### **** BERT 模型選出 \"quadratical\"（或包含它的那一段）***\n","\n","    它在原文中是對「equations」進行分類描述的關鍵詞\n","\n","    模型認為這段最有可能是對問題「What kind of equations...」的直接回答\n","\n","    預測方式是根據語意上下文和訓練資料中的問答模式做出的機率推論    "]},{"cell_type":"markdown","metadata":{"id":"vIXqnvTAr-yo"},"source":["## 6.7 總結(Summary)"]},{"cell_type":"markdown","metadata":{"id":"79H8d_zUr-yo"},"source":["\n","做得好！你成功地使用了大型語言模型(Large Language Model, LLM)從一系列句子中提取答案。儘管BERT在首次發布時是最先進的，但許多其他LLM自那以來已經取得了突破。[build.nvidia.com](https://build.nvidia.com/explore/discover)託管了許多這些模型，可以在瀏覽器中進行操作。去看看，看看今天的最先進技術在哪裡！"]},{"cell_type":"markdown","metadata":{"id":"CMETIc9Cr-yp"},"source":["\n","### 6.7.1 清空記憶體(Clear the Memory)\n","\n","在繼續之前，請執行以下程式碼區塊(Cell)以清空GPU記憶體。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tp1TeC6-r-yp"},"outputs":[],"source":["import IPython\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"]},{"cell_type":"markdown","metadata":{"id":"NLU1hfPCr-yp"},"source":["### 6.7.2 下一步(Next)"]},{"cell_type":"markdown","metadata":{"id":"botUt5SKr-yp"},"source":["\n","恭喜，你已完成課程的所有學習目標！\n","\n","作為最後的練習，並為了獲得課程認證，請在評量(Assessment)中從頭到尾的成功完成圖像分類問題。"]},{"cell_type":"markdown","metadata":{"id":"7PSohriXr-yp"},"source":["<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}